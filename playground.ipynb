{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["%load_ext autoreload\n","%autoreload 2\n","\n"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["from graph_reAct import graph\n","from langgraph.graph.state import CompiledStateGraph\n","from langchain_core.messages import HumanMessage\n"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["import logging\n","logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["import getpass\n","import os\n","\n","def _set_env(var: str):\n","    if not os.environ.get(var):\n","        os.environ[var] = getpass.getpass(f\"{var}: \")\n","        \n","_set_env(\"LANGCHAIN_TRACING_V2\")\n","_set_env(\"LANGCHAIN_ENDPOINT\")\n","_set_env(\"LANGCHAIN_API_KEY\")   \n","_set_env(\"LANGCHAIN_PROJECT\")\n","_set_env(\"OPENAI_API_KEY\")\n","_set_env(\"TAVILY_API_KEY\")   "]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-10-16 08:51:39,276 - INFO - message_value: messages=[SystemMessage(content=\"\\n    You are a tourguide bot. Initially give a very brief introduction to the place, but the immediately focus on answering the users questions.\\n    Your output will be converted to audio so don't include special characters in your answer, and pronounce abbreviations like ltd. and etc. as their full form.\\n    Respond to what the user said in a creative and helpful way.\\n    \\n    Here is some information about the item of interest:\\n    Name: London,\\n    \\n    Additional information from Wikipedia:\\n    Title: London\\n    Extract: London is the capital city of England.\\n    Description: London is the capital city of England.\\n    \\n    Please be nice and helpful and tell the user succinctly all about this place, incorporating both the basic information and the Wikipedia details.\\n    Answer any questions they have about the place, do not repeat yourself. \\n    \\n    Also, speak in old english.\\n    \", additional_kwargs={}, response_metadata={}), HumanMessage(content='Multiply 2 and 3', additional_kwargs={}, response_metadata={})]\n","2024-10-16 08:51:40,813 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n","2024-10-16 08:51:40,818 - INFO - content='' additional_kwargs={'tool_calls': [{'id': 'call_EF0DJaiAcWFlXmxI3HjZPe6i', 'function': {'arguments': '{\"a\":2,\"b\":3}', 'name': 'multiply'}, 'type': 'function'}], 'refusal': None} response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 359, 'total_tokens': 376, 'prompt_tokens_details': {'cached_tokens': 0}, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_e2bde53e6e', 'finish_reason': 'tool_calls', 'logprobs': None} id='run-7e7de8ba-2d70-4bd2-a1e3-271feb08f580-0' tool_calls=[{'name': 'multiply', 'args': {'a': 2, 'b': 3}, 'id': 'call_EF0DJaiAcWFlXmxI3HjZPe6i', 'type': 'tool_call'}] usage_metadata={'input_tokens': 359, 'output_tokens': 17, 'total_tokens': 376}\n","2024-10-16 08:51:40,849 - INFO - message_value: messages=[SystemMessage(content=\"\\n    You are a tourguide bot. Initially give a very brief introduction to the place, but the immediately focus on answering the users questions.\\n    Your output will be converted to audio so don't include special characters in your answer, and pronounce abbreviations like ltd. and etc. as their full form.\\n    Respond to what the user said in a creative and helpful way.\\n    \\n    Here is some information about the item of interest:\\n    Name: London,\\n    \\n    Additional information from Wikipedia:\\n    Title: London\\n    Extract: London is the capital city of England.\\n    Description: London is the capital city of England.\\n    \\n    Please be nice and helpful and tell the user succinctly all about this place, incorporating both the basic information and the Wikipedia details.\\n    Answer any questions they have about the place, do not repeat yourself. \\n    \\n    Also, speak in old english.\\n    \", additional_kwargs={}, response_metadata={}), HumanMessage(content='Multiply 2 and 3', additional_kwargs={}, response_metadata={}, id='7cc09697-fabf-4eb2-8df7-044605af3f5e'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_EF0DJaiAcWFlXmxI3HjZPe6i', 'function': {'arguments': '{\"a\":2,\"b\":3}', 'name': 'multiply'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 359, 'total_tokens': 376, 'prompt_tokens_details': {'cached_tokens': 0}, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_e2bde53e6e', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-7e7de8ba-2d70-4bd2-a1e3-271feb08f580-0', tool_calls=[{'name': 'multiply', 'args': {'a': 2, 'b': 3}, 'id': 'call_EF0DJaiAcWFlXmxI3HjZPe6i', 'type': 'tool_call'}], usage_metadata={'input_tokens': 359, 'output_tokens': 17, 'total_tokens': 376}), HumanMessage(content='6', additional_kwargs={}, response_metadata={})]\n"]},{"name":"stdout","output_type":"stream","text":["multiply\n"]},{"name":"stderr","output_type":"stream","text":["2024-10-16 08:51:41,341 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n"]},{"ename":"BadRequestError","evalue":"Error code: 400 - {'error': {'message': \"An assistant message with 'tool_calls' must be followed by tool messages responding to each 'tool_call_id'. The following tool_call_ids did not have response messages: call_EF0DJaiAcWFlXmxI3HjZPe6i\", 'type': 'invalid_request_error', 'param': 'messages.[3].role', 'code': None}}","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)","Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m thread_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m\n\u001b[0;32m----> 2\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m graph\u001b[38;5;241m.\u001b[39mainvoke({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: [(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMultiply 2 and 3\u001b[39m\u001b[38;5;124m\"\u001b[39m)]})\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m output[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:]:\n\u001b[1;32m      6\u001b[0m         m\u001b[38;5;241m.\u001b[39mpretty_print()\n","File \u001b[0;32m~/Repositories/tour-guide/backend/tour_guide_bot/.venv/lib/python3.11/site-packages/langgraph/pregel/__init__.py:1588\u001b[0m, in \u001b[0;36mPregel.ainvoke\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, **kwargs)\u001b[0m\n\u001b[1;32m   1586\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1587\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m-> 1588\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mastream(\n\u001b[1;32m   1589\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   1590\u001b[0m     config,\n\u001b[1;32m   1591\u001b[0m     stream_mode\u001b[38;5;241m=\u001b[39mstream_mode,\n\u001b[1;32m   1592\u001b[0m     output_keys\u001b[38;5;241m=\u001b[39moutput_keys,\n\u001b[1;32m   1593\u001b[0m     interrupt_before\u001b[38;5;241m=\u001b[39minterrupt_before,\n\u001b[1;32m   1594\u001b[0m     interrupt_after\u001b[38;5;241m=\u001b[39minterrupt_after,\n\u001b[1;32m   1595\u001b[0m     debug\u001b[38;5;241m=\u001b[39mdebug,\n\u001b[1;32m   1596\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   1597\u001b[0m ):\n\u001b[1;32m   1598\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1599\u001b[0m         latest \u001b[38;5;241m=\u001b[39m chunk\n","File \u001b[0;32m~/Repositories/tour-guide/backend/tour_guide_bot/.venv/lib/python3.11/site-packages/langgraph/pregel/__init__.py:1477\u001b[0m, in \u001b[0;36mPregel.astream\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[1;32m   1467\u001b[0m \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates\u001b[39;00m\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;66;03m# channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[1;32m   1469\u001b[0m \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[1;32m   1470\u001b[0m \u001b[38;5;66;03m# with channel updates applied only at the transition between steps\u001b[39;00m\n\u001b[1;32m   1471\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mtick(\n\u001b[1;32m   1472\u001b[0m     input_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_channels,\n\u001b[1;32m   1473\u001b[0m     interrupt_before\u001b[38;5;241m=\u001b[39minterrupt_before_,\n\u001b[1;32m   1474\u001b[0m     interrupt_after\u001b[38;5;241m=\u001b[39minterrupt_after_,\n\u001b[1;32m   1475\u001b[0m     manager\u001b[38;5;241m=\u001b[39mrun_manager,\n\u001b[1;32m   1476\u001b[0m ):\n\u001b[0;32m-> 1477\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner\u001b[38;5;241m.\u001b[39matick(\n\u001b[1;32m   1478\u001b[0m         loop\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m   1479\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_timeout,\n\u001b[1;32m   1480\u001b[0m         retry_policy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretry_policy,\n\u001b[1;32m   1481\u001b[0m         get_waiter\u001b[38;5;241m=\u001b[39mget_waiter,\n\u001b[1;32m   1482\u001b[0m     ):\n\u001b[1;32m   1483\u001b[0m         \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[1;32m   1484\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m output():\n\u001b[1;32m   1485\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m o\n","File \u001b[0;32m~/Repositories/tour-guide/backend/tour_guide_bot/.venv/lib/python3.11/site-packages/langgraph/pregel/runner.py:130\u001b[0m, in \u001b[0;36mPregelRunner.atick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[0m\n\u001b[1;32m    128\u001b[0m t \u001b[38;5;241m=\u001b[39m tasks[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 130\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m arun_with_retry(t, retry_policy, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_astream)\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n","File \u001b[0;32m~/Repositories/tour-guide/backend/tour_guide_bot/.venv/lib/python3.11/site-packages/langgraph/pregel/retry.py:102\u001b[0m, in \u001b[0;36marun_with_retry\u001b[0;34m(task, retry_policy, stream)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 102\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m task\u001b[38;5;241m.\u001b[39mproc\u001b[38;5;241m.\u001b[39mainvoke(task\u001b[38;5;241m.\u001b[39minput, config)\n\u001b[1;32m    103\u001b[0m \u001b[38;5;66;03m# if successful, end\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n","File \u001b[0;32m~/Repositories/tour-guide/backend/tour_guide_bot/.venv/lib/python3.11/site-packages/langgraph/utils/runnable.py:453\u001b[0m, in \u001b[0;36mRunnableSeq.ainvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    451\u001b[0m     coro \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39mainvoke(\u001b[38;5;28minput\u001b[39m, config)\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ASYNCIO_ACCEPTS_CONTEXT:\n\u001b[0;32m--> 453\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mcreate_task(coro, context\u001b[38;5;241m=\u001b[39mcontext)\n\u001b[1;32m    454\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    455\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mcreate_task(coro)\n","File \u001b[0;32m~/Repositories/tour-guide/backend/tour_guide_bot/.venv/lib/python3.11/site-packages/langgraph/utils/runnable.py:236\u001b[0m, in \u001b[0;36mRunnableCallable.ainvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ASYNCIO_ACCEPTS_CONTEXT:\n\u001b[1;32m    235\u001b[0m     coro \u001b[38;5;241m=\u001b[39m cast(Coroutine[\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, Any], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafunc(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n\u001b[0;32m--> 236\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mcreate_task(coro, context\u001b[38;5;241m=\u001b[39mcontext)\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    238\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafunc(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n","File \u001b[0;32m~/Repositories/tour-guide/backend/tour_guide_bot/graph.py:76\u001b[0m, in \u001b[0;36mcall_model\u001b[0;34m(state, config)\u001b[0m\n\u001b[1;32m     74\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage_value: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmessage_value\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# Get the model's response\u001b[39;00m\n\u001b[0;32m---> 76\u001b[0m response \u001b[38;5;241m=\u001b[39m cast(AIMessage, \u001b[38;5;28;01mawait\u001b[39;00m model\u001b[38;5;241m.\u001b[39mainvoke(message_value, config))\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# Handle the case when it's the last step and the model still wants to use a tool\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m state\u001b[38;5;241m.\u001b[39mis_last_step \u001b[38;5;129;01mand\u001b[39;00m response\u001b[38;5;241m.\u001b[39mtool_calls:\n","File \u001b[0;32m~/Repositories/tour-guide/backend/tour_guide_bot/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py:5325\u001b[0m, in \u001b[0;36mRunnableBindingBase.ainvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   5319\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mainvoke\u001b[39m(\n\u001b[1;32m   5320\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   5321\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[1;32m   5322\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   5323\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[1;32m   5324\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[0;32m-> 5325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbound\u001b[38;5;241m.\u001b[39mainvoke(\n\u001b[1;32m   5326\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   5327\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_configs(config),\n\u001b[1;32m   5328\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs},\n\u001b[1;32m   5329\u001b[0m     )\n","File \u001b[0;32m~/Repositories/tour-guide/backend/tour_guide_bot/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:307\u001b[0m, in \u001b[0;36mBaseChatModel.ainvoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mainvoke\u001b[39m(\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    300\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    305\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    306\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[0;32m--> 307\u001b[0m     llm_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magenerate_prompt(\n\u001b[1;32m    308\u001b[0m         [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[1;32m    309\u001b[0m         stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[1;32m    310\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    311\u001b[0m         tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    312\u001b[0m         metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    313\u001b[0m         run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    314\u001b[0m         run_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    315\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    316\u001b[0m     )\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ChatGeneration, llm_result\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mmessage\n","File \u001b[0;32m~/Repositories/tour-guide/backend/tour_guide_bot/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:796\u001b[0m, in \u001b[0;36mBaseChatModel.agenerate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21magenerate_prompt\u001b[39m(\n\u001b[1;32m    789\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    790\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    793\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    794\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    795\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 796\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magenerate(\n\u001b[1;32m    797\u001b[0m         prompt_messages, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    798\u001b[0m     )\n","File \u001b[0;32m~/Repositories/tour-guide/backend/tour_guide_bot/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:756\u001b[0m, in \u001b[0;36mBaseChatModel.agenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    743\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[1;32m    744\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\n\u001b[1;32m    745\u001b[0m             \u001b[38;5;241m*\u001b[39m[\n\u001b[1;32m    746\u001b[0m                 run_manager\u001b[38;5;241m.\u001b[39mon_llm_end(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    754\u001b[0m             ]\n\u001b[1;32m    755\u001b[0m         )\n\u001b[0;32m--> 756\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    757\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    758\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)  \u001b[38;5;66;03m# type: ignore[list-item, union-attr]\u001b[39;00m\n\u001b[1;32m    759\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[1;32m    760\u001b[0m ]\n\u001b[1;32m    761\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n","File \u001b[0;32m~/Repositories/tour-guide/backend/tour_guide_bot/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:932\u001b[0m, in \u001b[0;36mBaseChatModel._agenerate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    930\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    931\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_agenerate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 932\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_agenerate(\n\u001b[1;32m    933\u001b[0m             messages, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    934\u001b[0m         )\n\u001b[1;32m    935\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    936\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_agenerate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n","File \u001b[0;32m~/Repositories/tour-guide/backend/tour_guide_bot/.venv/lib/python3.11/site-packages/langchain_openai/chat_models/base.py:827\u001b[0m, in \u001b[0;36mBaseChatOpenAI._agenerate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    825\u001b[0m     generation_info \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mdict\u001b[39m(raw_response\u001b[38;5;241m.\u001b[39mheaders)}\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 827\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39masync_client\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpayload)\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m run_in_executor(\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_chat_result, response, generation_info\n\u001b[1;32m    830\u001b[0m )\n","File \u001b[0;32m~/Repositories/tour-guide/backend/tour_guide_bot/.venv/lib/python3.11/site-packages/openai/resources/chat/completions.py:1295\u001b[0m, in \u001b[0;36mAsyncCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, parallel_tool_calls, presence_penalty, response_format, seed, service_tier, stop, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m   1261\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m   1262\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m   1263\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1293\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m   1294\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m AsyncStream[ChatCompletionChunk]:\n\u001b[0;32m-> 1295\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[1;32m   1296\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1297\u001b[0m         body\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[1;32m   1298\u001b[0m             {\n\u001b[1;32m   1299\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[1;32m   1300\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[1;32m   1301\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[1;32m   1302\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m: function_call,\n\u001b[1;32m   1303\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m: functions,\n\u001b[1;32m   1304\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[1;32m   1305\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[1;32m   1306\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[1;32m   1307\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[1;32m   1308\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparallel_tool_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m: parallel_tool_calls,\n\u001b[1;32m   1309\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[1;32m   1310\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_format,\n\u001b[1;32m   1311\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[1;32m   1312\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mservice_tier\u001b[39m\u001b[38;5;124m\"\u001b[39m: service_tier,\n\u001b[1;32m   1313\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[1;32m   1314\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[1;32m   1315\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream_options\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream_options,\n\u001b[1;32m   1316\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[1;32m   1317\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[1;32m   1318\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[1;32m   1319\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_logprobs,\n\u001b[1;32m   1320\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[1;32m   1321\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[1;32m   1322\u001b[0m             },\n\u001b[1;32m   1323\u001b[0m             completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParams,\n\u001b[1;32m   1324\u001b[0m         ),\n\u001b[1;32m   1325\u001b[0m         options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[1;32m   1326\u001b[0m             extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[1;32m   1327\u001b[0m         ),\n\u001b[1;32m   1328\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mChatCompletion,\n\u001b[1;32m   1329\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1330\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mAsyncStream[ChatCompletionChunk],\n\u001b[1;32m   1331\u001b[0m     )\n","File \u001b[0;32m~/Repositories/tour-guide/backend/tour_guide_bot/.venv/lib/python3.11/site-packages/openai/_base_client.py:1826\u001b[0m, in \u001b[0;36mAsyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1812\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1813\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1814\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1821\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_AsyncStreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1822\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _AsyncStreamT:\n\u001b[1;32m   1823\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1824\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1825\u001b[0m     )\n\u001b[0;32m-> 1826\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls)\n","File \u001b[0;32m~/Repositories/tour-guide/backend/tour_guide_bot/.venv/lib/python3.11/site-packages/openai/_base_client.py:1519\u001b[0m, in \u001b[0;36mAsyncAPIClient.request\u001b[0;34m(self, cast_to, options, stream, stream_cls, remaining_retries)\u001b[0m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m   1511\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1512\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1517\u001b[0m     remaining_retries: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1518\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _AsyncStreamT:\n\u001b[0;32m-> 1519\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m   1520\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1521\u001b[0m         options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   1522\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1523\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1524\u001b[0m         remaining_retries\u001b[38;5;241m=\u001b[39mremaining_retries,\n\u001b[1;32m   1525\u001b[0m     )\n","File \u001b[0;32m~/Repositories/tour-guide/backend/tour_guide_bot/.venv/lib/python3.11/site-packages/openai/_base_client.py:1620\u001b[0m, in \u001b[0;36mAsyncAPIClient._request\u001b[0;34m(self, cast_to, options, stream, stream_cls, remaining_retries)\u001b[0m\n\u001b[1;32m   1617\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39maread()\n\u001b[1;32m   1619\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1620\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1622\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m   1623\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1624\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1627\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1628\u001b[0m )\n","\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': \"An assistant message with 'tool_calls' must be followed by tool messages responding to each 'tool_call_id'. The following tool_call_ids did not have response messages: call_EF0DJaiAcWFlXmxI3HjZPe6i\", 'type': 'invalid_request_error', 'param': 'messages.[3].role', 'code': None}}"]}],"source":["thread_id = 8\n","output = await graph.ainvoke({\"messages\": [(\"user\", \"Multiply 2 and 3\")]})\n","\n","\n","for m in output['messages'][-1:]:\n","        m.pretty_print()"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-10-14 21:05:56,640 - INFO - message_value: messages=[SystemMessage(content=\"\\n    You are a tourguide bot. Initially give a very brief introduction to the place, but the immediately focus on answering the users questions.\\n    Your output will be converted to audio so don't include special characters in your answer, and pronounce abbreviations like ltd. and etc. as their full form.\\n    Respond to what the user said in a creative and helpful way.\\n    \\n    Here is some information about the item of interest:\\n    Name: London,\\n    \\n    Additional information from Wikipedia:\\n    Title: London\\n    Extract: London is the capital city of England.\\n    Description: London is the capital city of England.\\n    \\n    Please be nice and helpful and tell the user succinctly all about this place, incorporating both the basic information and the Wikipedia details.\\n    Answer any questions they have about the place, do not repeat yourself. \\n    \\n    Also, speak in old english.\\n    \", additional_kwargs={}, response_metadata={}), HumanMessage(content='search about more about that fire!', additional_kwargs={}, response_metadata={})]\n","2024-10-14 21:05:57,355 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n","2024-10-14 21:05:57,552 - INFO - content='' additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_fNMr6td0gJ4tgmpj2LLtrVFZ', 'function': {'arguments': '{\"query\":\"London fire news\"}', 'name': 'search'}, 'type': 'function'}]} response_metadata={'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_e2bde53e6e'} id='run-aede622b-e989-4d5b-8d86-b4848cf7ccc2' tool_calls=[{'name': 'search', 'args': {'query': 'London fire news'}, 'id': 'call_fNMr6td0gJ4tgmpj2LLtrVFZ', 'type': 'tool_call'}]\n"]},{"name":"stdout","output_type":"stream","text":["|||||||||"]},{"name":"stderr","output_type":"stream","text":["2024-10-14 21:06:00,306 - INFO - message_value: messages=[SystemMessage(content=\"\\n    You are a tourguide bot. Initially give a very brief introduction to the place, but the immediately focus on answering the users questions.\\n    Your output will be converted to audio so don't include special characters in your answer, and pronounce abbreviations like ltd. and etc. as their full form.\\n    Respond to what the user said in a creative and helpful way.\\n    \\n    Here is some information about the item of interest:\\n    Name: London,\\n    \\n    Additional information from Wikipedia:\\n    Title: London\\n    Extract: London is the capital city of England.\\n    Description: London is the capital city of England.\\n    \\n    Please be nice and helpful and tell the user succinctly all about this place, incorporating both the basic information and the Wikipedia details.\\n    Answer any questions they have about the place, do not repeat yourself. \\n    \\n    Also, speak in old english.\\n    \", additional_kwargs={}, response_metadata={}), HumanMessage(content='search about more about that fire!', additional_kwargs={}, response_metadata={}, id='89acb076-a80f-4798-9324-c956646d093a'), AIMessage(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_fNMr6td0gJ4tgmpj2LLtrVFZ', 'function': {'arguments': '{\"query\":\"London fire news\"}', 'name': 'search'}, 'type': 'function'}]}, response_metadata={'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_e2bde53e6e'}, id='run-aede622b-e989-4d5b-8d86-b4848cf7ccc2', tool_calls=[{'name': 'search', 'args': {'query': 'London fire news'}, 'id': 'call_fNMr6td0gJ4tgmpj2LLtrVFZ', 'type': 'tool_call'}]), HumanMessage(content='[{\"url\": \"https://www.youtube.com/watch?v=TjBV4R18ICY\", \"content\": \"Assistant Commissioner of the London Fire Brigade tells Sky News that his crews are battling fires across the Capital.It comes after the UK recorded a temper...\"}, {\"url\": \"https://news.sky.com/story/london-and-surrey-fires-live-blazes-described-as-weather-related-people-urged-to-close-windows-and-doors-steady-progress-in-hayes-12658486\", \"content\": \"London Fire Brigade say the incident in Enfield is \\\\\"under control\\\\\". It adds \\\\\"grassland the size of 20 hectares was damaged by fire\\\\\", and it was attended by 100 firefighters. 17:55:51\"}, {\"url\": \"https://www.bbc.co.uk/news/live/uk-62184978\", \"content\": \"Accessibility links\\\\nNews Navigation\\\\nBreakingBreaking news\\\\nMedia player\\\\nAs it happened: Major incident declared in London as fires burn\\\\nRelated Video and Audio\\\\nPlay video Fields and buildings on fire in east London from BBCFields and buildings on fire in east London\\\\nPlay video Wildfires filmed near Bedford from BBCWildfires filmed near Bedford\\\\nPlay video Wildfire in Croydon, south London from BBCWildfire in Croydon, south London\\\\nPlay video UK forecast: Cooler temperatures ahead from BBCUK forecast: Cooler temperatures ahead\\\\nPlay video Keeping animals cool during the UK heatwave from BBCKeeping animals cool during the UK heatwave\\\\nPlay video Nasa graphic shows how the world has warmed from BBCNasa graphic shows how the world has warmed\\\\nPlay video Schools, transport should stay open in heatwave - UK PM from BBCSchools, transport should stay open in heatwave - UK PM\\\\nPlay video How was your sleep? \\\\\"\\\\nAre larger wildfires likely in the UK?\\\\nGeorgina Rannard\\\\nBBC News\\\\nAre the fires breaking out in the UK a sign of what is to come?\\\\nGuillermo Rein, Professor of Fire Science at Imperial College London, told me he expects that more powerful wildfires will gradually take place in the UK due to warmer\\\\nand drier vegetation.\\\\n with BBC Weatherâ€™s Nick Miller on BBC iPlayer now,\\\\nat bbc.co.uk/iplayer or on the BBC iPlayer app\\\\n\\'Boiling hot and tinder dry\\'\\\\nZoe Conway\\\\nReporting from Wennington, east London\\\\nA little earlier we reported that a man was stuck in his house in Wennington, east London and was digging trenches to stop the fire reaching his front door.\\\\n Animals rescued from sanctuary as fire burns nearby\\\\nHarry Low\\\\nReporting from Dartford\\\\nYou can smell it in the air, you can see it in the skyline - and the sweat soaked shirts of the firefighters bear testament to their hard work with 18 fire engines sent to this part of Dartford alone.\\\\n Greek authorities evacuate residents to the east of Athens\\\\nStephen Ryan\\\\nBBC News\\\\nGreek\\\\nauthorities have instructed people in four areas to the east of Athens near\\\\nPenteli to evacuate due to forest fires.\\\\n\"}, {\"url\": \"https://www.independent.co.uk/news/uk/home-news/london-fire-live-updates-dead-deaths-grenfell-tower-block-residents-latest-news-north-kensington-trapped-happening-damage-blaze-block-a7788836.html\", \"content\": \"London Fire Brigade has confirmed a \\\\\"number of fatalities\\\\\", however the number is not currently known owing to the \\\\\"size and complexity\\\\\" of the building. Katie Forster 14 June 2017 08:23 ...\"}, {\"url\": \"https://www.london-fire.gov.uk/news/\", \"content\": \"London Fire Brigade urges people to respect the water as hot weather continues. Firefighters are asking the public to respect the water as new figures show drownings in London in the first half of 2024 alone have already matched the total number in 2023. 31/07/2024 10:00 AM.\"}, {\"url\": \"https://www.nbcnews.com/news/world/fire-london-historic-somserset-house-rcna166998\", \"content\": \"The fire at London\\'s Somerset House has been brought under control with the help of around 125 firefighters, who were deployed Saturday to battle the blaze at the historic venue, known for housing ...\"}, {\"url\": \"https://www.cnn.com/2024/08/17/uk/blaze-firefighters-london-somerset-house-intl/index.html\", \"content\": \"A fire that broke out at London\\'s historic Somerset House on Saturday required around 125 firefighters to bring it under control, according to the London Fire Brigade. CNN values your feedback 1.\"}, {\"url\": \"https://www.bbc.com/news/uk-england-london-68443968\", \"content\": \"Eight police officers were among 11 people who were taken to hospital after a fire broke out at a five-storey building in London. About 160 people were evacuated from Emperor\\'s Gate, in South ...\"}, {\"url\": \"https://news.sky.com/story/major-incident-declared-across-london-after-huge-surge-in-fires-and-homes-destroyed-on-uks-hottest-ever-day-12655061\", \"content\": \"Major incident declared across London after \\'huge surge\\' in fires and homes destroyed on UK\\'s hottest-ever day. Dozens of fire engines and hundreds of firefighters have been involved in tackling ...\"}, {\"url\": \"https://www.newsnow.co.uk/h/UK/England/London/London+Fire+Brigade\", \"content\": \"Fires in Barking and Dagenham London Fire Brigade (Press Releases) 06:51 Thu, 03 Oct. Barking: 50 people evacuated from flats after fire BBC 00:22 Thu, 03 Oct. Monday. London Fire Brigade failed to learn lessons from Lakanal House fire, report says Safety & Health Practitioner Online 03:16 Tue, 01 Oct. 5 Oct 20:00.\"}]', additional_kwargs={}, response_metadata={})]\n"]},{"name":"stdout","output_type":"stream","text":["search\n"]},{"name":"stderr","output_type":"stream","text":["2024-10-14 21:06:00,488 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n"]},{"ename":"BadRequestError","evalue":"Error code: 400 - {'error': {'message': \"An assistant message with 'tool_calls' must be followed by tool messages responding to each 'tool_call_id'. The following tool_call_ids did not have response messages: call_fNMr6td0gJ4tgmpj2LLtrVFZ\", 'type': 'invalid_request_error', 'param': 'messages.[3].role', 'code': None}}","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)","Cell \u001b[0;32mIn[10], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m config \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfigurable\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthread_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: thread_id}}\n\u001b[1;32m      3\u001b[0m input_message \u001b[38;5;241m=\u001b[39m HumanMessage(content\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msearch about more about that fire!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m graph\u001b[38;5;241m.\u001b[39mastream_events({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: [input_message]}, config, version\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mv2\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# Get chat model tokens from a particular node \u001b[39;00m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m event[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevent\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_chat_model_stream\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m event[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlanggraph_node\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m==\u001b[39m node_to_stream:\n\u001b[1;32m      7\u001b[0m         data \u001b[38;5;241m=\u001b[39m event[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n","File \u001b[0;32m~/Repositories/tour-guide/backend/tour_guide_bot/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py:1377\u001b[0m, in \u001b[0;36mRunnable.astream_events\u001b[0;34m(self, input, config, version, include_names, include_types, include_tags, exclude_names, exclude_types, exclude_tags, **kwargs)\u001b[0m\n\u001b[1;32m   1372\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m   1373\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOnly versions \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mv1\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mv2\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m of the schema is currently supported.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1374\u001b[0m     )\n\u001b[1;32m   1376\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m aclosing(event_stream):\n\u001b[0;32m-> 1377\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m event_stream:\n\u001b[1;32m   1378\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m event\n","File \u001b[0;32m~/Repositories/tour-guide/backend/tour_guide_bot/.venv/lib/python3.11/site-packages/langchain_core/tracers/event_stream.py:1006\u001b[0m, in \u001b[0;36m_astream_events_implementation_v2\u001b[0;34m(runnable, input, config, include_names, include_types, include_tags, exclude_names, exclude_types, exclude_tags, **kwargs)\u001b[0m\n\u001b[1;32m   1004\u001b[0m \u001b[38;5;66;03m# Await it anyway, to run any cleanup code, and propagate any exceptions\u001b[39;00m\n\u001b[1;32m   1005\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1006\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m task\n\u001b[1;32m   1007\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mCancelledError:\n\u001b[1;32m   1008\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n","File \u001b[0;32m~/Repositories/tour-guide/backend/tour_guide_bot/.venv/lib/python3.11/site-packages/langchain_core/tracers/event_stream.py:966\u001b[0m, in \u001b[0;36m_astream_events_implementation_v2.<locals>.consume_astream\u001b[0;34m()\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    964\u001b[0m     \u001b[38;5;66;03m# if astream also calls tap_output_aiter this will be a no-op\u001b[39;00m\n\u001b[1;32m    965\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m aclosing(runnable\u001b[38;5;241m.\u001b[39mastream(\u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)) \u001b[38;5;28;01mas\u001b[39;00m stream:\n\u001b[0;32m--> 966\u001b[0m         \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m event_streamer\u001b[38;5;241m.\u001b[39mtap_output_aiter(run_id, stream):\n\u001b[1;32m    967\u001b[0m             \u001b[38;5;66;03m# All the content will be picked up\u001b[39;00m\n\u001b[1;32m    968\u001b[0m             \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    969\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n","File \u001b[0;32m~/Repositories/tour-guide/backend/tour_guide_bot/.venv/lib/python3.11/site-packages/langchain_core/tracers/event_stream.py:205\u001b[0m, in \u001b[0;36m_AstreamEventsCallbackHandler.tap_output_aiter\u001b[0;34m(self, run_id, output)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m cast(T, first)\n\u001b[1;32m    204\u001b[0m \u001b[38;5;66;03m# consume the rest of the output\u001b[39;00m\n\u001b[0;32m--> 205\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m output:\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send(\n\u001b[1;32m    207\u001b[0m         {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mevent, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchunk\u001b[39m\u001b[38;5;124m\"\u001b[39m: chunk}},\n\u001b[1;32m    208\u001b[0m         run_info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_type\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    209\u001b[0m     )\n\u001b[1;32m    210\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m chunk\n","File \u001b[0;32m~/Repositories/tour-guide/backend/tour_guide_bot/.venv/lib/python3.11/site-packages/langgraph/pregel/__init__.py:1477\u001b[0m, in \u001b[0;36mPregel.astream\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[1;32m   1467\u001b[0m \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates\u001b[39;00m\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;66;03m# channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[1;32m   1469\u001b[0m \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[1;32m   1470\u001b[0m \u001b[38;5;66;03m# with channel updates applied only at the transition between steps\u001b[39;00m\n\u001b[1;32m   1471\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mtick(\n\u001b[1;32m   1472\u001b[0m     input_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_channels,\n\u001b[1;32m   1473\u001b[0m     interrupt_before\u001b[38;5;241m=\u001b[39minterrupt_before_,\n\u001b[1;32m   1474\u001b[0m     interrupt_after\u001b[38;5;241m=\u001b[39minterrupt_after_,\n\u001b[1;32m   1475\u001b[0m     manager\u001b[38;5;241m=\u001b[39mrun_manager,\n\u001b[1;32m   1476\u001b[0m ):\n\u001b[0;32m-> 1477\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner\u001b[38;5;241m.\u001b[39matick(\n\u001b[1;32m   1478\u001b[0m         loop\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m   1479\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_timeout,\n\u001b[1;32m   1480\u001b[0m         retry_policy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretry_policy,\n\u001b[1;32m   1481\u001b[0m         get_waiter\u001b[38;5;241m=\u001b[39mget_waiter,\n\u001b[1;32m   1482\u001b[0m     ):\n\u001b[1;32m   1483\u001b[0m         \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[1;32m   1484\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m output():\n\u001b[1;32m   1485\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m o\n","File \u001b[0;32m~/Repositories/tour-guide/backend/tour_guide_bot/.venv/lib/python3.11/site-packages/langgraph/pregel/runner.py:130\u001b[0m, in \u001b[0;36mPregelRunner.atick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[0m\n\u001b[1;32m    128\u001b[0m t \u001b[38;5;241m=\u001b[39m tasks[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 130\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m arun_with_retry(t, retry_policy, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_astream)\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n","File \u001b[0;32m~/Repositories/tour-guide/backend/tour_guide_bot/.venv/lib/python3.11/site-packages/langgraph/pregel/retry.py:99\u001b[0m, in \u001b[0;36marun_with_retry\u001b[0;34m(task, retry_policy, stream)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[0;32m---> 99\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m task\u001b[38;5;241m.\u001b[39mproc\u001b[38;5;241m.\u001b[39mastream(task\u001b[38;5;241m.\u001b[39minput, config):\n\u001b[1;32m    100\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n","File \u001b[0;32m~/Repositories/tour-guide/backend/tour_guide_bot/.venv/lib/python3.11/site-packages/langgraph/utils/runnable.py:576\u001b[0m, in \u001b[0;36mRunnableSeq.astream\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    574\u001b[0m output: Any \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    575\u001b[0m add_supported \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 576\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m aiterator:\n\u001b[1;32m    577\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m chunk\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;66;03m# collect final output\u001b[39;00m\n","File \u001b[0;32m~/Repositories/tour-guide/backend/tour_guide_bot/.venv/lib/python3.11/site-packages/langchain_core/tracers/event_stream.py:182\u001b[0m, in \u001b[0;36m_AstreamEventsCallbackHandler.tap_output_aiter\u001b[0;34m(self, run_id, output)\u001b[0m\n\u001b[1;32m    180\u001b[0m tap \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_tapped\u001b[38;5;241m.\u001b[39msetdefault(run_id, sentinel)\n\u001b[1;32m    181\u001b[0m \u001b[38;5;66;03m# wait for first chunk\u001b[39;00m\n\u001b[0;32m--> 182\u001b[0m first \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m py_anext(output, default\u001b[38;5;241m=\u001b[39msentinel)\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first \u001b[38;5;129;01mis\u001b[39;00m sentinel:\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n","File \u001b[0;32m~/Repositories/tour-guide/backend/tour_guide_bot/.venv/lib/python3.11/site-packages/langchain_core/utils/aiter.py:78\u001b[0m, in \u001b[0;36mpy_anext.<locals>.anext_impl\u001b[0;34m()\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21manext_impl\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[T, Any]:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     73\u001b[0m         \u001b[38;5;66;03m# The C code is way more low-level than this, as it implements\u001b[39;00m\n\u001b[1;32m     74\u001b[0m         \u001b[38;5;66;03m# all methods of the iterator protocol. In this implementation\u001b[39;00m\n\u001b[1;32m     75\u001b[0m         \u001b[38;5;66;03m# we're relying on higher-level coroutine concepts, but that's\u001b[39;00m\n\u001b[1;32m     76\u001b[0m         \u001b[38;5;66;03m# exactly what we want -- crosstest pure-Python high-level\u001b[39;00m\n\u001b[1;32m     77\u001b[0m         \u001b[38;5;66;03m# implementation and low-level C anext() iterators.\u001b[39;00m\n\u001b[0;32m---> 78\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;21m__anext__\u001b[39m(iterator)\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopAsyncIteration\u001b[39;00m:\n\u001b[1;32m     80\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m default\n","File \u001b[0;32m~/Repositories/tour-guide/backend/tour_guide_bot/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py:1444\u001b[0m, in \u001b[0;36mRunnable.atransform\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   1441\u001b[0m final: Input\n\u001b[1;32m   1442\u001b[0m got_first_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 1444\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m ichunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28minput\u001b[39m:\n\u001b[1;32m   1445\u001b[0m     \u001b[38;5;66;03m# The default implementation of transform is to buffer input and\u001b[39;00m\n\u001b[1;32m   1446\u001b[0m     \u001b[38;5;66;03m# then call stream.\u001b[39;00m\n\u001b[1;32m   1447\u001b[0m     \u001b[38;5;66;03m# It'll attempt to gather all input into a single chunk using\u001b[39;00m\n\u001b[1;32m   1448\u001b[0m     \u001b[38;5;66;03m# the `+` operator.\u001b[39;00m\n\u001b[1;32m   1449\u001b[0m     \u001b[38;5;66;03m# If the input is not addable, then we'll assume that we can\u001b[39;00m\n\u001b[1;32m   1450\u001b[0m     \u001b[38;5;66;03m# only operate on the last chunk,\u001b[39;00m\n\u001b[1;32m   1451\u001b[0m     \u001b[38;5;66;03m# and we'll iterate until we get to the last chunk.\u001b[39;00m\n\u001b[1;32m   1452\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m got_first_val:\n\u001b[1;32m   1453\u001b[0m         final \u001b[38;5;241m=\u001b[39m ichunk\n","File \u001b[0;32m~/Repositories/tour-guide/backend/tour_guide_bot/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py:1444\u001b[0m, in \u001b[0;36mRunnable.atransform\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   1441\u001b[0m final: Input\n\u001b[1;32m   1442\u001b[0m got_first_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 1444\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m ichunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28minput\u001b[39m:\n\u001b[1;32m   1445\u001b[0m     \u001b[38;5;66;03m# The default implementation of transform is to buffer input and\u001b[39;00m\n\u001b[1;32m   1446\u001b[0m     \u001b[38;5;66;03m# then call stream.\u001b[39;00m\n\u001b[1;32m   1447\u001b[0m     \u001b[38;5;66;03m# It'll attempt to gather all input into a single chunk using\u001b[39;00m\n\u001b[1;32m   1448\u001b[0m     \u001b[38;5;66;03m# the `+` operator.\u001b[39;00m\n\u001b[1;32m   1449\u001b[0m     \u001b[38;5;66;03m# If the input is not addable, then we'll assume that we can\u001b[39;00m\n\u001b[1;32m   1450\u001b[0m     \u001b[38;5;66;03m# only operate on the last chunk,\u001b[39;00m\n\u001b[1;32m   1451\u001b[0m     \u001b[38;5;66;03m# and we'll iterate until we get to the last chunk.\u001b[39;00m\n\u001b[1;32m   1452\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m got_first_val:\n\u001b[1;32m   1453\u001b[0m         final \u001b[38;5;241m=\u001b[39m ichunk\n","File \u001b[0;32m~/Repositories/tour-guide/backend/tour_guide_bot/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py:1006\u001b[0m, in \u001b[0;36mRunnable.astream\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    988\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mastream\u001b[39m(\n\u001b[1;32m    989\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    990\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[1;32m    991\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    992\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[1;32m    993\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m AsyncIterator[Output]:\n\u001b[1;32m    994\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    995\u001b[0m \u001b[38;5;124;03m    Default implementation of astream, which calls ainvoke.\u001b[39;00m\n\u001b[1;32m    996\u001b[0m \u001b[38;5;124;03m    Subclasses should override this method if they support streaming output.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1004\u001b[0m \u001b[38;5;124;03m        The output of the Runnable.\u001b[39;00m\n\u001b[1;32m   1005\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1006\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mainvoke(\u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n","File \u001b[0;32m~/Repositories/tour-guide/backend/tour_guide_bot/.venv/lib/python3.11/site-packages/langgraph/utils/runnable.py:236\u001b[0m, in \u001b[0;36mRunnableCallable.ainvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ASYNCIO_ACCEPTS_CONTEXT:\n\u001b[1;32m    235\u001b[0m     coro \u001b[38;5;241m=\u001b[39m cast(Coroutine[\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, Any], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafunc(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n\u001b[0;32m--> 236\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mcreate_task(coro, context\u001b[38;5;241m=\u001b[39mcontext)\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    238\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafunc(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n","File \u001b[0;32m~/Repositories/tour-guide/backend/tour_guide_bot/graph.py:76\u001b[0m, in \u001b[0;36mcall_model\u001b[0;34m(state, config)\u001b[0m\n\u001b[1;32m     74\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage_value: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmessage_value\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# Get the model's response\u001b[39;00m\n\u001b[0;32m---> 76\u001b[0m response \u001b[38;5;241m=\u001b[39m cast(AIMessage, \u001b[38;5;28;01mawait\u001b[39;00m model\u001b[38;5;241m.\u001b[39mainvoke(message_value, config))\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# Handle the case when it's the last step and the model still wants to use a tool\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m state\u001b[38;5;241m.\u001b[39mis_last_step \u001b[38;5;129;01mand\u001b[39;00m response\u001b[38;5;241m.\u001b[39mtool_calls:\n","File \u001b[0;32m~/Repositories/tour-guide/backend/tour_guide_bot/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py:5325\u001b[0m, in \u001b[0;36mRunnableBindingBase.ainvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   5319\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mainvoke\u001b[39m(\n\u001b[1;32m   5320\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   5321\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[1;32m   5322\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   5323\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[1;32m   5324\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[0;32m-> 5325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbound\u001b[38;5;241m.\u001b[39mainvoke(\n\u001b[1;32m   5326\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   5327\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_configs(config),\n\u001b[1;32m   5328\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs},\n\u001b[1;32m   5329\u001b[0m     )\n","File \u001b[0;32m~/Repositories/tour-guide/backend/tour_guide_bot/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:307\u001b[0m, in \u001b[0;36mBaseChatModel.ainvoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mainvoke\u001b[39m(\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    300\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    305\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    306\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[0;32m--> 307\u001b[0m     llm_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magenerate_prompt(\n\u001b[1;32m    308\u001b[0m         [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[1;32m    309\u001b[0m         stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[1;32m    310\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    311\u001b[0m         tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    312\u001b[0m         metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    313\u001b[0m         run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    314\u001b[0m         run_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    315\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    316\u001b[0m     )\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ChatGeneration, llm_result\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mmessage\n","File \u001b[0;32m~/Repositories/tour-guide/backend/tour_guide_bot/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:796\u001b[0m, in \u001b[0;36mBaseChatModel.agenerate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21magenerate_prompt\u001b[39m(\n\u001b[1;32m    789\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    790\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    793\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    794\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    795\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 796\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magenerate(\n\u001b[1;32m    797\u001b[0m         prompt_messages, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    798\u001b[0m     )\n","File \u001b[0;32m~/Repositories/tour-guide/backend/tour_guide_bot/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:756\u001b[0m, in \u001b[0;36mBaseChatModel.agenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    743\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[1;32m    744\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\n\u001b[1;32m    745\u001b[0m             \u001b[38;5;241m*\u001b[39m[\n\u001b[1;32m    746\u001b[0m                 run_manager\u001b[38;5;241m.\u001b[39mon_llm_end(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    754\u001b[0m             ]\n\u001b[1;32m    755\u001b[0m         )\n\u001b[0;32m--> 756\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    757\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    758\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)  \u001b[38;5;66;03m# type: ignore[list-item, union-attr]\u001b[39;00m\n\u001b[1;32m    759\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[1;32m    760\u001b[0m ]\n\u001b[1;32m    761\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n","File \u001b[0;32m~/Repositories/tour-guide/backend/tour_guide_bot/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:920\u001b[0m, in \u001b[0;36mBaseChatModel._agenerate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_stream(\n\u001b[1;32m    915\u001b[0m     async_api\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    916\u001b[0m     run_manager\u001b[38;5;241m=\u001b[39mrun_manager,\n\u001b[1;32m    917\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    918\u001b[0m ):\n\u001b[1;32m    919\u001b[0m     chunks: List[ChatGenerationChunk] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 920\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_astream(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    921\u001b[0m         chunk\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mresponse_metadata \u001b[38;5;241m=\u001b[39m _gen_info_and_msg_metadata(chunk)\n\u001b[1;32m    922\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_manager:\n","File \u001b[0;32m~/Repositories/tour-guide/backend/tour_guide_bot/.venv/lib/python3.11/site-packages/langchain_openai/chat_models/base.py:1989\u001b[0m, in \u001b[0;36mChatOpenAI._astream\u001b[0;34m(self, stream_usage, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1986\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream_usage:\n\u001b[1;32m   1987\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream_options\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minclude_usage\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream_usage}\n\u001b[0;32m-> 1989\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m_astream(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1990\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m chunk\n","File \u001b[0;32m~/Repositories/tour-guide/backend/tour_guide_bot/.venv/lib/python3.11/site-packages/langchain_openai/chat_models/base.py:776\u001b[0m, in \u001b[0;36mBaseChatOpenAI._astream\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    774\u001b[0m     base_generation_info \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mdict\u001b[39m(raw_response\u001b[38;5;241m.\u001b[39mheaders)}\n\u001b[1;32m    775\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 776\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39masync_client\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpayload)\n\u001b[1;32m    777\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m response:\n\u001b[1;32m    778\u001b[0m     is_first_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n","File \u001b[0;32m~/Repositories/tour-guide/backend/tour_guide_bot/.venv/lib/python3.11/site-packages/openai/resources/chat/completions.py:1295\u001b[0m, in \u001b[0;36mAsyncCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, parallel_tool_calls, presence_penalty, response_format, seed, service_tier, stop, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m   1261\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m   1262\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m   1263\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1293\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m   1294\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m AsyncStream[ChatCompletionChunk]:\n\u001b[0;32m-> 1295\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[1;32m   1296\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1297\u001b[0m         body\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[1;32m   1298\u001b[0m             {\n\u001b[1;32m   1299\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[1;32m   1300\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[1;32m   1301\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[1;32m   1302\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m: function_call,\n\u001b[1;32m   1303\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m: functions,\n\u001b[1;32m   1304\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[1;32m   1305\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[1;32m   1306\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[1;32m   1307\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[1;32m   1308\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparallel_tool_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m: parallel_tool_calls,\n\u001b[1;32m   1309\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[1;32m   1310\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_format,\n\u001b[1;32m   1311\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[1;32m   1312\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mservice_tier\u001b[39m\u001b[38;5;124m\"\u001b[39m: service_tier,\n\u001b[1;32m   1313\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[1;32m   1314\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[1;32m   1315\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream_options\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream_options,\n\u001b[1;32m   1316\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[1;32m   1317\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[1;32m   1318\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[1;32m   1319\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_logprobs,\n\u001b[1;32m   1320\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[1;32m   1321\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[1;32m   1322\u001b[0m             },\n\u001b[1;32m   1323\u001b[0m             completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParams,\n\u001b[1;32m   1324\u001b[0m         ),\n\u001b[1;32m   1325\u001b[0m         options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[1;32m   1326\u001b[0m             extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[1;32m   1327\u001b[0m         ),\n\u001b[1;32m   1328\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mChatCompletion,\n\u001b[1;32m   1329\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1330\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mAsyncStream[ChatCompletionChunk],\n\u001b[1;32m   1331\u001b[0m     )\n","File \u001b[0;32m~/Repositories/tour-guide/backend/tour_guide_bot/.venv/lib/python3.11/site-packages/openai/_base_client.py:1826\u001b[0m, in \u001b[0;36mAsyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1812\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1813\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1814\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1821\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_AsyncStreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1822\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _AsyncStreamT:\n\u001b[1;32m   1823\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1824\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1825\u001b[0m     )\n\u001b[0;32m-> 1826\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls)\n","File \u001b[0;32m~/Repositories/tour-guide/backend/tour_guide_bot/.venv/lib/python3.11/site-packages/openai/_base_client.py:1519\u001b[0m, in \u001b[0;36mAsyncAPIClient.request\u001b[0;34m(self, cast_to, options, stream, stream_cls, remaining_retries)\u001b[0m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m   1511\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1512\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1517\u001b[0m     remaining_retries: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1518\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _AsyncStreamT:\n\u001b[0;32m-> 1519\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m   1520\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1521\u001b[0m         options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   1522\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1523\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1524\u001b[0m         remaining_retries\u001b[38;5;241m=\u001b[39mremaining_retries,\n\u001b[1;32m   1525\u001b[0m     )\n","File \u001b[0;32m~/Repositories/tour-guide/backend/tour_guide_bot/.venv/lib/python3.11/site-packages/openai/_base_client.py:1620\u001b[0m, in \u001b[0;36mAsyncAPIClient._request\u001b[0;34m(self, cast_to, options, stream, stream_cls, remaining_retries)\u001b[0m\n\u001b[1;32m   1617\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39maread()\n\u001b[1;32m   1619\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1620\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1622\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m   1623\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1624\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1627\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1628\u001b[0m )\n","\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': \"An assistant message with 'tool_calls' must be followed by tool messages responding to each 'tool_call_id'. The following tool_call_ids did not have response messages: call_fNMr6td0gJ4tgmpj2LLtrVFZ\", 'type': 'invalid_request_error', 'param': 'messages.[3].role', 'code': None}}"]}],"source":["node_to_stream = \"call_model\"\n","config = {\"configurable\": {\"thread_id\": thread_id}}\n","input_message = HumanMessage(content=\"search about more about that fire!\")\n","async for event in graph.astream_events({\"messages\": [input_message]}, config, version=\"v2\"):\n","    # Get chat model tokens from a particular node \n","    if event[\"event\"] == \"on_chat_model_stream\" and event['metadata'].get('langgraph_node','') == node_to_stream:\n","        data = event[\"data\"]\n","        print(data[\"chunk\"].content, end=\"|\")"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"data":{"image/jpeg":"/9j/4AAQSkZJRgABAQAAAQABAAD/4gHYSUNDX1BST0ZJTEUAAQEAAAHIAAAAAAQwAABtbnRyUkdCIFhZWiAH4AABAAEAAAAAAABhY3NwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAQAA9tYAAQAAAADTLQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAlkZXNjAAAA8AAAACRyWFlaAAABFAAAABRnWFlaAAABKAAAABRiWFlaAAABPAAAABR3dHB0AAABUAAAABRyVFJDAAABZAAAAChnVFJDAAABZAAAAChiVFJDAAABZAAAAChjcHJ0AAABjAAAADxtbHVjAAAAAAAAAAEAAAAMZW5VUwAAAAgAAAAcAHMAUgBHAEJYWVogAAAAAAAAb6IAADj1AAADkFhZWiAAAAAAAABimQAAt4UAABjaWFlaIAAAAAAAACSgAAAPhAAAts9YWVogAAAAAAAA9tYAAQAAAADTLXBhcmEAAAAAAAQAAAACZmYAAPKnAAANWQAAE9AAAApbAAAAAAAAAABtbHVjAAAAAAAAAAEAAAAMZW5VUwAAACAAAAAcAEcAbwBvAGcAbABlACAASQBuAGMALgAgADIAMAAxADb/2wBDAAMCAgMCAgMDAwMEAwMEBQgFBQQEBQoHBwYIDAoMDAsKCwsNDhIQDQ4RDgsLEBYQERMUFRUVDA8XGBYUGBIUFRT/2wBDAQMEBAUEBQkFBQkUDQsNFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBT/wAARCAD5ANYDASIAAhEBAxEB/8QAHQABAAMAAwEBAQAAAAAAAAAAAAUGBwMECAIBCf/EAFYQAAEEAQIDAgYLCwgFDQAAAAEAAgMEBQYRBxIhEzEIFBUiQZQWFzJRVVZhddHS0yMzNlRxdIGTlbKzJjU3QlKRobQYYnKx8AkkJURFU2NkgqKjwcP/xAAbAQEAAgMBAQAAAAAAAAAAAAAAAQIDBAUGB//EADURAQABAgEIBgoDAQEAAAAAAAABAhEDBBIhMVFxkdEFFEFSYaETFSIyM2KBkrHwI0LB4fH/2gAMAwEAAhEDEQA/AP6poiICIiAiIgIiICIuG7cgx1Sa1ZkbDXhaXvkd3NA7ypiL6IHMunbzOPoP5LV6tWf/AGZpmtP+JUEzF3dXtFjJy28djHjeLFRPMMj2/wBqd7Tzbn/u2kADo7m7h3aeh9OUGclfA42Fvp5KkYJ9PU7dTv13Kz5mHTornT4c/wB3p0drseyrC/DFD1ln0p7KsL8MUPWWfSnsVwvwPQ9WZ9CexXC/A9D1Zn0J/D4+SdB7KsL8MUPWWfSnsqwvwxQ9ZZ9KexXC/A9D1Zn0J7FcL8D0PVmfQn8Pj5Gg9lWF+GKHrLPpT2VYX4Yoess+lPYrhfgeh6sz6E9iuF+B6HqzPoT+Hx8jQ/W6owziA3L0ST6BZZ9KkIpWTRh8b2yMd3Oadwf0qNOlMI4EHD0CD0INVn0LoSaBxcEhnxLHYC5uD22M2ia7bps+PbkeNunnNPyEEApbBnVMx+/u00LIihsJmLEtmXGZONkWVgYJC6FpENmMnYSxbkkDfo5hJLCQCSCxzplYaqZom0qiIiqCIiAiIgIiICIiAiIgIiICrGqNsnqHT+FcA6CV8mQnYd/PZByco/Wywu/9O3pVnVZzDfFNeacuOB7Oatcx+4G4539lM3c+jpXk/wCNlsYHv38J/EpjWsyIi10Co7+NmjGa+OixmDJqNsjYH1oak8kccjmc7Y3zNYYmPLevK5wO3oV4XmzOeWNN+EHFLoHB6tpz5bNVm6nht44nAXqvYgSXY5z0jmY0NaOVwL3M2LD3kLhws8JnT3EeDWE01e7h4tO27wlls4+2yI1Kzw0zOkfC1rXkecYd+do7x0JVj0lx70LrirmZ8PnDN5HreOXobFKxWmig2ce17KWNr3M2a7ZzWkHbosdxWR1zobTHGjTWn9NZqLWc+Zy+cwmQOOdJQnjneJIzHOfubpdnECNx35m7EbKs4DTmRn4i57KY/C8Q7uNyHD7IYoZPV0Fl8094SMk7MRyedFu0nlAaxjncwYCg1LW3hf6MwehWak0+67qWtJboV2SQYy62BzbMobziXsC1xa0SEsHXnYIzs9wC2XT2fp6pwtTK0PGPE7TeePxqrLWl23I86KVrXtPTuc0Fef8AVWi82fAy0Vi6GBuT5jD0NPXJ8NFAW2j4tLVlnjEZ2PaARv8ANPUkbd63nSWpotYYCtlYaGSxkc5dtVy9N9SyzlcW+dE8Bzd9txuOoIKCYREQVfXu2Po0s2zZs+LtRyF3vwveI5m/KORxOx6czWn0Ai0KscRx2+lJ6LdzLkJYaTABuSZJGtJ/IAXOPyAqzrYq04VMztnho/2ZT2CIi10CIiAiIgIiICIiAiIgIiICjs/hY89jXVnSGCRr2TQztG7opWODmPH5CBuPSNwehUiitTVNMxVGuBB4bUgs2RjMkI6OcY3d9Xm82YDvkhJ92z/Fu+ztiqrL4OHCqeV8snDnS8kj3FznuxMBLie8k8qvGXwlDPVfFshUitwg8zRI3ctd6HNPe0j3xsVCjQjYQW1c9nake2wYLxmDfyGUPP8Ais1sKvTfNnjHP91p0Sr7vBs4UOcS7hvpZzj1JOJgJP8A7Vf8djquHx9WhRrxU6VWJsEFeBgZHFG0BrWNaOgAAAAHvKA9hNj41Z79dD9knsJsfGrPfrofsk9Hh9/yktG1aEVX9hNj41Z79dD9kqnw6x+V1TRzct7VOYD6eav0IuxlhA7KGdzGb/cz53KBv8voCejw+/5SWja1RU7VXBzQmusoMlqLR+DzmQEYi8ayFCKaTkG+zeZzSdhuenyrs+wmx8as9+uh+yT2E2PjVnv10P2Sejw+/wCUlo2q9/o1cJt9/a20t+yIPqqy6f0rpThbhLMOGxeK0tiTKbEzKkMdaEyENbzuAAG5DWjf5AFxjRE/p1TnnD3u3iH+6PddmhofF0rkVyVs+SuxEOjsZGw+w6M7bbsDyWsO2/VoHeffKZuFGuq+6Of/AE0OLHwS6ky1bMWYH16NQOOOrzMcyUvcC1072nblJaS1rSNw1zy7q7lbZERY668+fCCRERY0CIiAiIgIiICIiAiIgIiICIiAiIgIiICz3guQcVqjlJP8p8t3+/40/wCU/wDHvLQlnvBffyVqjfb8J8t7kD8af7yDQkREBERAREQEREBERAREQEREBERAREQEREBERAREQEREBZ5wVG2J1T5wd/KjLdw/809aGs84K7eSdU7df5UZbvG3/WnoNDREQEREBERAREQEREBERAREQEREBERAREQEUFqLUsmLsQ0aFVt/KTsdK2GSTso44xsC+R4a7lG5AAAJJ32GzXFsMc7q/c7UMJt6N7c32a2aMnrrjO0RvmE2XZFSPLusPxDB+tzfZp5d1h+IYP1ub7NX6rXtjjBZd0VI8u6w/EMH63N9mnl3WH4hg/W5vs06rXtjjBZy8Yte3OF/DPUGq6GDk1JPia4tHGxT9i6WMOb2hD+V23Kwuf3Hfl26b7rzH4GXhdW+M2uMvpahoWSlUmt3s5cyjskHtqMllL2M5BC3nJe5re8el3o2PpOxldV268sE+MwM0MrSySOSzMWuaRsQQYuoIWUeD5wHt+DnS1DBgKeJsvzN91qSaxal544hv2UA2j6tYC7r6SSU6rXtjjBZ6SRUjy7rD8Qwfrc32aeXdYfiGD9bm+zTqte2OMFl3RUjy7rD8Qwfrc32aeXdYfiGD9bm+zTqte2OMFl3RUjy7rD8Qwfrc32a+m6wzeJBsZrGURjWdZrGPsvkfC30vLHRjdo7yQdwPQU6ridlp+sFl1RfjXB7Q5pDmkbgjuK/VpoEREBERAREQEREBERAREQUa2d+JuSHoGHpkfpmtb/7gpZRFr+k7KfM9L+PaUuutV/XdH4hMiIiogRQ+S1dicRqPDYK3b7LK5hs7qNfs3u7YQta6XzgC1uwc0+cRvv03TTOrsTrGrcs4e345DTuz4+d3ZvZyTwvMcrNnAb8rmkbjoduhIUCYREUgiIgIofTersTq+PIvxNvxtuPvTY2yezezs7ETuWRnnAb7HpuNwfQSmpdXYnSEeOky1vxRmQvQY2seze/tLEzuWNnmg7bnpudgPSQoEwo3UwB03lgQCDUl6H/AGCpJRupvwcyv5pL+4Vko9+N6Y1rDpdxdpnEEncmnCST/sBSii9K/gxh/wAzh/cClFzcT36t5OsREWNAiIgIiICIiAiIgIiIKLa/pOynzPS/j2lLqItf0nZT5npfx7Sl11qv67o/EJliWvn5HXnHzHaBk1BldPYCvpx2ce3CW3U7F+c2exDDM3zwyNo5i1hG5kbv0Gyh+KWLuWM5pvh9p7Jaxyuep4uW/JPDqd2LY2uZBG2azYbG98snOC1rQ0gjmLh3FaxrzhRpXiYaD9RYrxyeg5zqlqGxLWsQFw2cGSxOa8A7DcB2x2G46KJyHg/aBylPEVZ8DtFioH1avY3LETuxe7mfFI5kgdKxzupbIXAkncdVgmJQxnhvq3Ka4zHg1ZvN2BbytrGZttix0BleyGNhcdum55Nzt6d1pXgykexjWI9I1nnQfk/59IrLBwP0RVxmAx8GBjhqYC+7JYqOOeVppTueXuMZDt2sLid49+Qjpy7dFx2uHFnTd/K5LQBw2nslmrXjeWkyVKxcisyAEB7Y2WImseSSXOHuvT1G6REwIbwm8pk8TwxilxOUuYa7Lm8VXFyjJySsbJehY/Y9QQWuIIIIIOxBB2WYZHRuRi15xTwEOutZxY3B6fq5fHM8vTufBalZY5nGQkve0GBpEbnFnnO83u22UaE1DqyN1DX97AZ7CtkhtRVsVjbNCRtmGZksTzIbcm7WuZvy7Dc7bnbcGwyaCwU2ZzmVfR3v5unHj783bSfdoIxIGM25tm7drJ1aAfO6noNkxfSMA0Llc5x81DiKGc1RmsBUraIw+aEWBvOoSXLVxj3S2HPZsXNYYw0M9zuTuConh3rjUnG5/DbS2Y1PksTUnw2SyV3J4ec07OYfVu+KRBsrNnMBZ92dybc249C3LOeD9oHUWLwePu4EmvhaLcZRdBdsQSx1GtDRA6WORr5GbNG7XucD3nckrt6k4J6I1ZhsHishgIRTwewxgpSyVH0hyhu0UkLmPYCAAQDsdhvvsozZHljCZLU+HfU4fadvXJm5rXepI7N2fMux9u0KvK5sZuNhkcx7+YuJa0Od2ZALdyrjqLT2uNL4PTFXWFpk9A8QsFJio5Ms/KWa8ZlAkjksvhic8c43bzAkB2xJ2C2r/R74fexCTTB05GcK6+7KCE2JjJHacdzNHLz9pG75WOG2598rte0fog6Hs6Qfg2zYCzP41NBPZmkkfNu0iUzOeZOcFrdnc242HVIpkXpRupvwcyv5pL+4VIRRthjZG3cNaA0bnfoPlKj9Tfg5lfzSX9wrYo9+N6Y1rDpX8GMP+Zw/uBSii9K/gxh/zOH9wKUXNxPfq3k6xERY0CIiAiIgIiICIiAiIgotr+k7KfM9L+PaUuuHUuFvMyseaxcTbc4g8WsU3ychljBLmFjj0Dmlzuh6EOPUbDeLdlc81xA0dlHAHvFmnsf/AJ11otiUxMTGqI0zEatHata6aRVn2WZV18U2aPy8s+7mv7Kao9sTg1ruWRwm5WEte0gOIJB3G67flbPfEzK+tUvt1OZ80fdHMsm0UJ5Wz3xMyvrVL7dPK2e+JmV9apfbpmfNH3RzLJtFCeVs98TMr61S+3XTxmrMtl4531tG5ktgnkrP7WSrGedji12wdMNxuOjh0PeCQmZ80fdHMss6KE8rZ74mZX1ql9unlbPfEzK+tUvt0zPmj7o5lk2ihPK2e+JmV9apfbrDsl4eHDXCalvYDJnJ43LUbD6lmvcriLspWuLXNLi7l6EHrvsmZ80fdHMs9GKN1N+DmV/NJf3Co7HanymXx9W/Q0teu0bUTZ4LNe7RkimjcA5r2OFghzSCCCOhBXNNVz+pq82OfhZsHXsMMU1y3Yhc5jCNndm2J7937dBuQBvv125TammKZiqaotHjHMiFs0r+DGH/ADOH9wKUXHXgZVgjhibyRRtDGtHoAGwC5FyKpzqplUREVQREQEREBERAREQERdHL5eHC1BPNHPMXSMiZDVhdLI973BrQGtBO27hu47NaN3OLWtJAdm3bgoVZrNmaOvWhY6SWaVwayNgG5c4noAACSSq/4xe1fA8VXz4nC2K9eevkoXhlufmIe9nZSRnsmlmzCXbSbvdsIyxrj2amFs3b8GRy8jHWqk9g1IakkjYWRv8ANYXtJ2fJyD3RHmmR4b06mcQdXH4uniY5Y6NSCnHLNJYkZXjawPle4ue8gDq5ziXE95JJK7SIgIiICr+jZxPWyn3fJWOTJ2mE5KPkc3aQ+bH0G8Q7mH0t2VgVc0pOPKWp6ptZGy+tk+vj7OVjA+CGQNgd/WiHPtv6Hc4/qoLGiIgLyZxZ8A7HcU/CRrcQZMjWx+CdXZYvUBXbO+1fiIEfNG9hjMLgGmQO3LuUt2+6FzfWaIKhpzWVxmUi09qitDjtQOa50E1YuNPJNbuS+BzurX7DmdC4lzOuxkYO0db1Gaj01jNW4mXGZenHepSFrix+4LHtIcyRjhs5j2uAc17SHNcAWkEAqs0szktB3IMbqOyb+EnkENLUMpAfG9xPLBbAAa0nzWsmHR7iGODXlnaheUREBERAREQEREBERAREQR+ZzlTBw13WpmxyWp2VK0Z5iZZnnZrAGgn3ySAeVrXOOzWkjgw2Hkgndksh2UmZnhZDO+uX9k1rS5zWMa4nYAvO7uhcdidgGtb1sbLLlNW5Sxz5SvXxrRj2154hFVne5kczp4z3y7BzI+buaWSNHXmVgQEREBERAREQFXjI/G65DXSZSxDlaYDGCPnpVZIHEk8w6sklbMOh80iv02PurCujmsRHm6Pi0k9mttJHMyapMYpGPY8PaQ4ejdoBad2uaS1wLSQQ7yKKwWZkyDHVr0VejmoWNks4+K02cxNc97WSAgAljzG8tc5rSeU7gEECVQEREBcNupBfqzVbUMdmtMx0csMrQ5kjCNi1wPQggkEFcyIKJoG7Y0/nMloe9PJZ8nQx28VankL5Z6DyWtY9ziXPfC9pjLjuXNMTnEuc4q9rPtYPNLi9w8ssfyusxZLHPbufOY6KOfu7ujqze/5du8rQUBERAREQEREBERARFHah1JidI4exls7lKWFxVfl7a9kbDIIIuZwa3me8ho3c5oG56kgelBG6JPPVyrv+meuVt9M17sbSkfcf/A6bx/6pB9KsayzhPxo4f6xs3cVgteUc7lJsneMVKxlq81p4bK9x7FjHlxgABLCB97AK1NAREQEREBERAREQdDK4s32skgn8SvxbdlbZG17mt5mucw8w6sfyAOA2JHcWuDXD4xeZ8fklr2K76F6N8g8Vme0ufG15a2VvKSCxw2cPSOYBwa4ECSWAeGNW4qWtAwRcJsJHazkrZobWZhtMhv4+s4N52VuYtPNJsAXB27ezBDeblfGG5YbNY/UWMgyOKv1snj7ALobdOZs0UgBIJa9pIPUEdD6F3V4r/wCTx4lTaV4V5TRet3u09Ywl1zqJyp7FskMpJcxhdsDyyNfvt/bC9V+2lo7404j12P6VsdXxu5PCU2nYtKKre2lo7404j12P6U9tLR3xpxHrsf0p1fG7k8JTmzsQ+qmm9xn0DVazmFWjlci52x83lFeAfJufGTtv37HbuK0FY9jOIWmbvGfO5SfPY2KjRw1ShTnktRhsskks0tjkO/UANrA7HvB95Xj20tHfGnEeux/SnV8buTwkzZ2LSiq3tpaO+NOI9dj+lTGG1FitRRSSYvJVMiyMgPdVnbIGk9QDyk7fpVasHEoi9VMxG5FphIoiLCgRFlXFHiPYrXJcBhZjBPGB47eZ7qLcAiKP3nkEEu/qgjbznbs2smybEyrEjDw//Bes5rXA6akEWTy9OlMRzCGSUdoR74Z7rb5dlCnjNo0H+emfogl+qsKhrRwFxY3z3kue9xLnvJO5LnHqTv6SuReqo6DwIj265mfC0f5JeG4+3No34ab6vL9RVriVqnh1xR0FndJ5fLtdj8tVfWkIrSkxk9WyDzfdNcGuHytCzNFf1Hk3eq4xyLwyTwBuF+F4H5LVepNY3YYM9JM7F45oikeBWa7d8zSGnpIQ3buIDD085ezPbm0b8NN9Xl+osORPUeTd6rjHIvDcfbm0b8NN9Xl+ouetxb0faeGjUFSInuNgmEf3vACwdCA4EEbg94KT0Hk/ZVV5ci8PUcUrJ4mSRvbJG8BzXsO4cD3EFfa816S1Ld0Lb7bG7vpOcXT4zn5Ypd+8tHcx/p3G2/8AW37x6HwuYqagxVbI0ZRNVsM52O/3gj0EHcEeggrzmXZBXkVUXm9M6p5m53URFywREQFUeIj+3iwWMkO9TJ5HxezHt0ljbBNMWO/1XGEAjuIJaQQSFblTuIH866L+eH/5C2trJvix9fKJTGt3WMbG0NY0NaBsABsAv1EWygREQEREBQWfLcblMJk4B2Vvx+Go6RvQyRSvDHMd746h3XfYtBHVTqgNX/e8L88Uv47VlwtNcRtWp1tCREXHVcF202jTnsv9xDG6R35AN/8A6XljHzy3Krbc7uezbJszP225nvPM4/3leqLtVt6nPWf7iaN0bvyEbFeV6FeWlWbTsN5LNQmtMzfflew8rv8AEL1vQObbE26P9J1OwihdRavx2ljXF9t93b83J4ljrFvu235uxjfy9479t+u3cVD+23p/lJ7LObA7fg7kPsF6ecSimbTVF96jtcR+IFDhtpwZS8GvMs8dWvE+ZkLZJnnzQ6R5DWN2BJcTsA0nr3LP4vCTpjD6hnlx1OzkMPWhuGDEZeK9BPE+URnlmYByvaT1a5o729djuJrWDavGDGVquCsXKGZw9yHLU5criLUFcyxkgNf2sbOZrg9zSGncb77dF86l0Vq/W3D3UGGycenqGQuiFlUY98xiaGyNc8ySOYCd+XoAzp8vetLErxqqpnCnRbRaIm82nt3pdyLizLicplqWq8MNPuo4p2abJFbFpslZruV++zW8sjTyjlHMDzDZxVbh13qjP8SeHXj2DsaaxOQ8dlZF5REhss8WLmCeJoAa4dHAEu2PpBCsWuuFcuudUX7E9mKDFXNNWcI8tJMzJZJo3teG7bFoDD6d99unpURW0traLP6TzWqJsI+hpiK06V+KFiWxZDq5jDxF2fuugJY3fvO2/QKtfps7NmZtExs06Yvf6bhrqKmji3p8n71nP06dyA//AAX1DxXwFiaOJkWb53uDRzafyDRuT6SYNh+Urd9Nh96OKFwWncBsm8x6gxJJMdaaK3GD3NEwcC0fJzxPd+V5WYrT+A2MeI8/lyCI7M0dSInue2EOJcPk55Xt/Kwrm9LZvU687wtvvyuvT2tXREXz8EREBU7iB/Oui/nh/wDkLauKp3ED+ddF/PD/APIW1tZL8T6T+JTDvLKPCe1lqXQXCC/l9KiJuTZcpwumkmEZijksxscW7xvDi7mDO4bB5cDu0A6uqRxr0Ba4n8Ms1pujbio37QhlrWJ2l0bZYpmTM5wOvKXRgHbrsSs86tCEBnOLupMVksBpmto2tkteZKpPkLGKgzHLTpVY5AztX2nQgnmLmAARb7kjoBuYuPwkjk8Rg6+I0tYua2ymTuYj2OWLbIRVsVNzaMtjZzRGwcpDmtcXdozZu52H7ktC8SJtUYPXlNulotY18fYw9/GSWbJoWKr5WSxuZN2XaNe17N+sZBDiOneoTH+D5qnTMOB1Ni8tibvECnmclmbrbjZYsdaN9obPA0tDpI2tDIeR2zj9z6t87YV9ocOqOMWT1O/SFM1rmktQY3X9HDZvFw3edrmPgklDe0ZyiWGRjmOG4G+3VvRXfS/GTJ6w4n57TWN0vG7E4K8cffyU+VjjsxPEIkEgqcnMYnFwa1/N1J322B2pV7wf9YXsbd1FJlMK/iFa1TU1O6IiYY0CtD2EVXn27TlEW+8nLuXH3KnNRcJ9Xar4yYDU1iPTGJo4XIeMR5jG9uMtaqdm5ppzAtDCxznbk85GwGzQd1GkbYoDV/3vC/PFL+O1T6gNX/e8L88Uv47VtYPxIWp1w0JERcdUWWcUeG9m5ckz2Eh7ew8Dx2k3o6bYACWP33gAAtPugBtsW7P1NFtZNlOJkuJGJh6/yPKsVmOZz2NcRIw7PieC17D3EOaerT8hC5F6NzmjcFqV4flMRTvSgbCWaFpeB7wd3j+9Qh4N6NP/AGHF+iWT6y9VR05gzHt0TE+Fp5FoYai3L2m9G/AcX62T6ye03o34Di/WyfWV/XmTd2rhHMtDDUW5e03o34Di/WyfWT2m9G/AcX62T6yevMm7tXCOZaGGo5wa0kkADqSfQty9pvRvwHF+tk+suetwm0fVe17dPUpS3qPGI+2A9Pc/cKJ6cyfspq8uZaGNaR0xe11aEeO5o6AJE2T5N4o9u8MJ6Pf6NhuB/W9APoXDYipgMXWx9GIQ1a7AyNg973yfSSepPpJJXbjjZDG1jGhjGgNa1o2AA7gAvpedy7L68tqi8WpjVHNO4REXLQIiICp3ED+ddF/PD/8AIW1cVUeIjBBDg8pICKmLyHjNmT0RRugmhL3dPctMoJPcACSQAVtZN8WPr5xKY1u2i+Y5GTMa9jmvY4bhzTuCvpbKBERAREQFAav+94X54pfx2qfUFneXJ5XC4yA9rbF6G2+NnUxxROD3Pd7w6ADfbcuACy4WiuJ2LU62gIiLjqiIiAiIgIiICIiAiIgIiICIiAiIgIiIK1Pw00jakMk2l8PK897nUIie/f8As++SuP2q9GfFPCfs+L6qtKLY6xjR/eeMpvO1Vvar0Z8U8J+z4vqp7VejPinhP2fF9VWlE6xjd+eMl52qt7VejPinhP2fF9VPar0Z8U8J+z4vqq0onWMbvzxkvO1Vvar0Z8U8J+z4vqqZw+n8Xp6F8WLxtTGxPILmVIGxBxHQEhoG6kEVasbEri1VUzG8vIiIsKH/2Q==","text/plain":["<IPython.core.display.Image object>"]},"metadata":{},"output_type":"display_data"}],"source":["from IPython.display import Image, display\n","display(Image(graph.get_graph(xray=True).draw_mermaid_png()))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":".venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"}},"nbformat":4,"nbformat_minor":2}
